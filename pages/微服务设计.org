#+TITLE: 微服务设计

** 
:PROPERTIES:
:last_modified_at: 1609337624066
:background_color: #533e7d
:created_at: 1609233078964
:publication_date: 
:author: [[张三]]
:source: [[weread]]
:type: [[book]]
:END:
** ***1.1 什么是微服务

有一个黄金法则是：你是否能够修改一个服务并对其进行部署，而不影响其他任何服务？

***1.2 主要好处

在一个由多个服务相互协作的系统中，可以在不同的服务中使用最适合该服务的技术。尝试使用一种适合所有场景的标准化技术，会使得所有的场景都无法得到很好的支持

弹性工程学的一个关键概念是舱壁。如果系统中的一个组件不可用了，但并没有导致级联故障，那么系统的其他部分还可以正常运行。服务边界就是一个很显然的舱壁。在单块系统中，如果服务不可用，那么所有的功能都会不可用。对于单块服务的系统而言，可以通过将同样的实例运行在不同的机器上来降低功能完全不可用的概率，然而微服务系统本身就能够很好地处理服务不可用和功能降级问题

***1.3 面向服务的架构

SOA（Service-Oriented Architecture，面向服务的架构）是一种设计方法，其中包含多个服务，而服务之间通过配合最终会提供一系列功能。一个服务通常以独立的形式存在于操作系统进程中。服务之间通过网络调用，而非采用进程内调用的方式进行通信。

***2.4 一个原则性的方法

[插图]图2-1：原则和实践的真实例子

***2.5 要求的标准

能够清晰地描绘出跨服务系统的健康状态非常关键。这必须在系统级别而非单个服务级别进行考虑。

你可以使用Graphite来收集指标数据，使用Nagios来检测健康状态，或者使用轮询系统来从各个节点收集数据，但无论你的选择是什么，都应尽量保持标准化。每个服务内的技术应该对外不透明，并且不要为了服务的具体实现而改变监控系统。日志功能和监控情况类似：也需要集中式管理。

对以下几种请求做不同的处理可以帮助系统及时失败，并且也很容易追溯问题：（1）正常并且被正确处理的请求；（2）错误请求，并且服务识别出了它是错误的，但什么也没做；（3）被访问的服务宕机了，所以无法判断请求是否正常。

***2.8 例外管理

有时候我们会决定针对某个规则破一次例，然后把它记录下来。如果这样的例外出现了很多次，就可以通过修改原则和实践的方式把我们的理解固化下来。举个例子，可能我们有一个实践论述应该总是使用MySQL做数据存储，但是后来有足够的证明表明在海量存储的场景下应使用Cassandra，这时就可以对实践进行修改：“在大多数场景下使用MySQL做存储，如果是数据快速增长的场景，可以使用Cassandra。”

***2.10 建设团队

对于技术领导人来说，更重要的事情是帮助你的队友成长，帮助他们理解这个愿景，并保证他们可以积极地参与到愿景的实现和调整中来。

***3.3 限界上下文

他认为任何一个给定的领域都包含多个限界上下文，每个限界上下文中的东西（Eric更常使用模型这个词，应该比“东西”好得多）分成两部分，一部分不需要与外部通信，另一部分则需要。每个上下文都有明确的接口，该接口决定了它会暴露哪些模型给其他的上下文。

[插图]图3-1：财务部门和仓库之间共享的模型

尽管在仓库内部有相应的模型来表示库存项，但是我们不会直接把这个模型暴露出去。也就是对该模型来说，存在内部和外部两种表示方式。

所以一旦你发现了领域内部的限界上下文，一定要使用模块对其进行建模，同时使用共享和隐藏模型。

一般来讲，微服务应该清晰地和限界上下文保持一致。熟练之后，就可以省掉在单块系统中先使用模块的这个步骤，而直接使用单独的服务。然而对于一个新系统而言，可以先使用一段时间的单块系统，因为如果服务之间的边界搞错了，后面修复的代价会很大。所以最好能够等到系统稳定下来之后，再确定把哪些东西作为一个服务划分出去。

***3.4 业务功能

当你在思考组织内的限界上下文时，不应该从共享数据的角度来考虑，而应该从这些上下文能够提供的功能来考虑

***3.5 逐步划分上下文

当考虑微服务的边界时，首先考虑比较大的、粗粒度的那些上下文，然后当发现合适的缝隙后，再进一步划分出那些嵌套的上下文

***3.8 小结

限界上下文是寻找这些接缝的一个非常重要的工具，通过将微服务与这些边界相匹配，可以保证最终的系统能够得到微服务提供的所有好处。

***4.5 编排与协同

使用编排（orchestration）的话，我们会依赖于某个中心大脑来指导并驱动整个流程，就像管弦乐队中的指挥一样。使用协同（choreography）的话，我们仅仅会告知系统中各个部分各自的职责，而把具体怎么做的细节留给它们自己，就像芭蕾舞中每个舞者都有自己的方式，同时也会响应周围其他人。

***4.6 远程过程调用

。你应该做出一个假设：有一些恶意的攻击者随时有可能对网络进行破坏，因此网络的出错模式也不止一种。服务端可能会返回一个错误信息，或者是请求本身就是有问题的。你能够区分出不同的故障模式吗？如果可以，分别如何处理？

***4.9 服务即状态机

把关键领域的生命周期显式建模出来非常有用。我们不但可以在唯一的一个地方处理状态冲突（比如，尝试更新已经被移除的用户），而且可以在这些状态变化的基础上封装一些行为。

***4.11 微服务世界中的DRY和代码重用的危险

如果你想要使用客户端库，一定要保证其中只包含处理底层传输协议的代码，比如服务发现和故障处理等。千万不要把与目标服务相关的逻辑放到客户端库中

Netflix的客户端库会处理类似服务发现、故障模式、日志等方面的工作，可以看到这些方面与服务本身的职责并没有什么关系。如果不使用这些共享客户端，Netflix就很难保证客户端和服务器之间的通信能够在规模化的情况下正常工作。

***4.15 与第三方软件集成

核心思想是，任何定制化都只在自己可控的平台上进行，并限制工具的消费者的数量

***4.16 小结

无论如何避免数据库集成· 理解REST和RPC之间的取舍，但总是使用REST作为请求/响应模式的起点· 相比编排，优先选择协同· 避免破坏性修改、理解Postel法则、使用容错性读取器· 将用户界面视为一个组合层

***5.11 重构数据库

Scott J. Ambler和Pramod J. Sadalage编写的《数据库重构》。

***5.12 事务边界

，再发起一个补偿事务来抵消之前的操作。对于我们来说，可能就是简单的一个DELETE操作来把订单从数据库中删除。然后还需要向用户报告该操作失败了

在这种情况下，你要么重试补偿事务，要么使用一些后台任务来清除这些不一致的状态。可以给后台的维护人员提供一个界面来进行该操作，或者将其自动化。

处理分布式事务（尤其是上面处理客户订单这类的短事务）常用的算法是两阶段提交。在这种方式中，首先是投票阶段。在这个阶段，每个参与者（在这个上下文中叫作cohort）会告诉事务管理器它是否应该继续。如果事务管理器收到的所有投票都是成功，则会告知它们进行提交操作。只要收到一个否定的投票，事务管理器就会让所有的参与者回退。

***6.1 持续集成简介

CI能够保证新提交的代码与已有代码进行集成，从而让所有人保持同步。CI服务器会检测到代码已提交并签出，然后花些时间来验证代码是否通过编译以及测试能否通过。

***6.13 小结

专注于保持服务能够独立于其他服务进行部署的能力，无论采用什么技术，请确保它能够提供这个能力。我倾向于一个服务一个代码库，对于每个微服务一个CI这件事情，我不仅仅是倾向，并且非常坚持，因为只有这样才能实现独立部署。

Jez Humble和David Farley的《持续交付》

***7.2 测试范围

Cohn在他的原始模型中把自动化测试划分为单元测试、服务测试和用户界面测试三层。[插图]图7-2:Mike Cohn的测试金字塔。出自Mike Cohn的《Scrum敏捷软件开发》第1版，经过Pearson出版社的许可进行了修改

服务测试是绕开用户界面、直接针对服务的测试。在独立应用程序中，服务测试可能只测试为用户界面提供服务的一些类。对于包含多个服务的系统，一个服务测试只测试其中一个单独服务的功能。

既然所有的测试都有优缺点，那每种类型需要占多大的比例呢？一个好的经验法则是，顺着金字塔向下，下面一层的测试数量要比上面一层多一个数量级。

***7.3 实现服务测试

我们还需要配置被测服务，在测试过程中连接这些打桩服务。接着，为了模仿真实的服务，我们需要配置打桩服务为被测服务的请求发回响应

打桩，是指为被测服务的请求创建一些有着预设响应的打桩服务

弗里曼和普雷斯的书《测试驱动的面向对象软件开发》。

***7.10 部署后再测试

当考虑使用金丝雀发布时，你需要选择是要引导部分生产请求到金丝雀，还是直接复制一份生产请求。有些团队选择先复制一份生产请求，然后引导复制的请求到金丝雀。使用这种方法，现运行的生产版本和金丝雀版本可以有相同的请求，只是生产环境的请求结果是外部可见的。这方便大家对新旧版本做比较，同时又避免假如金丝雀失败，影响到客户的请求。不过，复制生产请求的工作可能会很复杂，尤其是在事件/请求不是幂等的情况下。

***8.1 单一服务，单一服务器

我们希望监控主机本身。CPU、内存等所有这些主机的数据都有用。我们想知道，系统健康的时候它们应该是什么样子的，这样当它们超出边界值时，就可以发出警告。如果我们想运行自己的监控软件，可以使用Nagios，或者使用像New Relic这样的托管服务来帮助我们监控主机。

我们甚至可以更进一步，使用logrotate帮助我们移除旧的日志，避免日志占满了磁盘空间。

***8.2 单一服务，多个服务器

单一服务的实例运行在多个主机上在这种情况下，我们依然想追踪有关主机的数据，根据它们来发出警告。但现在，除了要查看所有主机的数据，还要查看单个主机自己的数据。换句话说，我们既想把数据聚合起来，又想深入分析每台主机。Nagios允许以这样的方式组织我们的主机，到目前为止一切还好。类似的方式也可以满足我们对应用程序的监控。

***8.5 多个服务的指标跟踪

Graphite就是一个让上述要求变得很容易的系统。它提供一个非常简单的API，允许你实时发送指标数据给它。然后你可以通过查看这些指标生成的图表和其他展示方式来了解当前的情况。它处理容量的方式很有趣．。通过有效地配置，它可以减少旧指标的精度，以确保容量不要太大。例如，最近的十分钟，每隔10秒记录一次主机CPU的指标，然后在过去的一天，以分钟为单位对数据进行聚合，而在过去的几年，减少到以30分钟为单位进行聚合。通过这种方式，你不需要大量的存储空间，就可以保存很长一段时间内的信息。

***8.6 服务指标

我强烈建议你公开自己服务的基本指标。作为Web服务，最低限度应该暴露如响应时间和错误率这样的一些指标。

***8.8 关联标识

一个非常有用的方法是使用关联标识（ID）。在触发第一个调用时，生成一个GUID。然后把它传递给所有的后续调用，如图8-5所示。类似日志级别和日期，你也可以把关联标识以结构化的方式写入日志。使用合适的日志聚合工具，你能够对事件在系统中触发的所有调用进行跟踪：        15-02-2014 16:01:01 Web-Frontend INFO [abc-123] Register        15-02-2014 16:01:02 RegisterService INFO [abc-123] RegisterCustomer ...        15-02-2014 16:01:03 PostalSystem INFO [abc-123] SendWelcomePack ...        15-02-2014 16:01:03 EmailSystem INFO [abc-123] SendWelcomeEmail ...        15-02-2014 16:01:03 PaymentGateway ERROR [abc-123] ValidatePayment ...[插图]图8-5：使用关联标识来跟踪跨多个服务的调用当然，你需要确保每个服务知道应该传递关联标识。此时你需要标准化，强制在系统中执行该标准。一旦这样做了，你就可以创建工具来跟踪各种交互。这样的工具可以用于跟踪事件风暴、不常发生的特殊场景，甚至识别出时间过长的事务，因为你能勾勒出整个级联的调用。

***8.13 小结

对每个服务而言，· 最低限度要跟踪请求响应时间。做好之后，可以开始跟踪错误率及应用程序级的指标。· 最低限度要跟踪所有下游服务的健康状态，包括下游调用的响应时间，最好能够跟踪错误率。一些像Hystrix这样的库，可以在这方面提供帮助。· 标准化如何收集指标以及存储指标。· 如果可能的话，以标准的格式将日志记录到一个标准的位置。如果每个服务各自使用不同的方式，聚合会非常痛苦！· 监控底层操作系统，这样你就可以跟踪流氓进程和进行容量规划。对系统而言，· 聚合CPU之类的主机层级的指标及应用程序级指标。· 确保你选用的指标存储工具可以在系统和服务级别做聚合，同时也允许你查看单台主机的情况。· 确保指标存储工具允许你维护数据足够长的时间，以了解你的系统的趋势。· 使用单个可查询工具来对日志进行聚合和存储。· 强烈考虑标准化关联标识的使用。· 了解什么样的情况需要行动，并根据这些信息构造相应的警报和仪表盘。· 调查对各种指标聚合方式做统一化的可能性，像Suro或Riemann这样的工具可能会对你有用。

***9.1 身份验证和授权

当主体试图访问一个资源（比如基于Web的接口）时，他会被定向到一个身份提供者那里进行身份验证。这个身份提供者会要求他提供用户名和密码，或使用更先进的双重身份验证。一旦身份提供者确认主体已通过身份验证，它会发消息给服务提供者，让服务

对于企业来说，通常有自己的身份提供者，它会连接到公司的目录服务。目录服务可能使用LDAP（Lightweight Directory Access Protocol，轻量级目录访问协议）或活动目录（Active Directory）。

这些系统允许你存储主体的信息，例如他们在组织中扮演什么样的角色。

***9.2 服务间的身份验证和授权

SSL之上的流量不能被反向代理服务器（比如Varnish或Squid）所缓存，这是使用HTTPS的另一个缺点

这意味着，如果你需要缓存信息，就不得不在服务端或客户端内部实现。你可以在负载均衡中把Https的请求转成Http的请求，然后在负载均衡之后就可以使用缓存了。

***9.4 深度防御

IDS（Intrusion Detection Systems，入侵检测系统）可以监控网络或主机，当发现可疑行为时报告问题。IPS（Intrusion Prevention Systems，入侵预防系统），也会监控可疑行为，并进一步阻止它的发生。

***10.10 案例研究：RealEstate.com.au

在业务线之间，所有通信都必须是异步批处理，这是非常小的架构团队的几个严格的规则之一。这种粗粒度的通信与不同业务之间的粗粒度的通信是匹配的。坚持异步批处理，每条业务线在自身的行为和管理上有很大的自由度。它可以随时停止其服务，只要能满足其他业务线的批量集成，以及自己业务干系人的需求，那么没有人会在意。

***11.2 多少是太多

响应时间/延迟各种操作需要多长时间？我们可以使用不同数量的用户来测量它，以了解负载的增加会对响应时间造成什么样的影响。鉴于网络的性质，你经常会遇到异常值，所以将监控的响应目标设置成一个给定的百分比是很有用的。目标还应该包括你期望软件处理的并发连接/用户数。所以，你可能会说：“我期望这个网站，当每秒处理200个并发连接时，90%的响应时间在2秒以内。”· 可用性你能接受服务出现故障吗？这是一个24/7服务吗？当测量可用性时，有些人喜欢查看可接受的停机时间，但这个对调用服务的人又有什么用呢？对于你的服务，我只能选择信赖或者不信赖。测量停机时间，只有从历史报告的角度才有用。· 数据持久性多大比例的数据丢失是可以接受的？数据应该保存多久？很有可能每个案例都不同。例如，你可能为了节省空间，选择将用户会话的日志只保存一年，但你的金融交易记录可能需要保存很多年。

***11.3 功能降级

构建一个弹性系统，尤其是当功能分散在多个不同的、有可能宕掉的微服务上时，重要的是能够安全地降级功能。

我们需要做的是理解每个故障的影响，并弄清楚如何恰当地降级功能

对于每个使用多个微服务的面向用户的界面，或每个依赖多个下游合作者的微服务来说，你都需要问自己：“如果这个微服务宕掉会发生什么？”然后你就知道该做什么了。

***11.4 架构性安全措施

正确地设置超时，实现舱壁隔离不同的连接池，并实现一个断路器，以便在第一时间避免给一个不健康的系统发送调用。

***11.5 反脆弱的组织

给所有的跨进程调用设置超时，并选择一个默认的超时时间。当超时发生后，记录到日志里看看发生了什么，并相应地调整它们

使用断路器时，当对下游资源的请求发生一定数量的失败后，断路器会打开。接下来，所有的请求在断路器打开的状态下，会快速地失败。一段时间后，客户端发送一些请求查看下游服务是否已经恢复，如果它得到了正常的响应，将重置断路器。你可以在图11-2中看到这个过程的概述。[插图]

[插图]图11-3：每个下游服务一个连接池，以提供舱壁

Hystrix允许你在一定条件下，实现拒绝请求的舱壁，以避免资源达到饱和，这被称为减载（load shedding）。有时拒绝请求是避免重要系统变得不堪重负或成为多个上游服务瓶颈的最佳方法。

***11.6 幂等

这种机制在基于事件的协作中也会工作得很好，尤其是当你有多个相同类型的服务实例都订阅同一个事件时，会非常有用。即使我们存储了哪些事件被处理过，在某些形式的异步消息传递中，可能还留有小窗口，两个worker会看到相同的信息。通过以幂等方式处理这些事件，我们确保不会导致任何问题。

这里的关键点是，我们认为那些业务操作是幂等的，而不是整个系统状态的。

***11.8 扩展数据库

服务可以在单个主节点上进行所有的写操作，但是读取被分发到一个或多个只读副本。从主数据库复制到副本，是在写入后的某个时刻完成的，这意味着使用这种技术读取，有时候看到的可能是失效的数据，但是最终能够读取到一致的数据，这样的方式被称为最终一致性。如果你能够处理暂时的不一致，这是一个相当简单和常见的用来扩展系统的方式。稍后我们在看CAP定理时，会深入讨论这个话题

扩展读取是比较容易的。那么扩展写操作呢？一种方法是使用分片。采用分片方式，会存在多个数据库节点。当你有一块数据要写入时，对数据的关键字应用一个哈希函数，并基于这个函数的结果决定将数据发送到哪个分片

，内部用于处理命令和查询的模型本身是完全独立的。例如，我可能选择把命令作为事件，只是将命令列表存储在一个数据存储中（这一过程称为事件溯源，event sourcing）。我的查询模型可以查询事件库，从存储的事件推算出领域对象的状态，或只是从系统的命令部分获取一个聚合，来更新其他不同类型的存储。在许多方面，我们得到跟之前讨论的只读副本方式同样的好处，但CQRS中的副本数据，不需要和处理数据修改的数据存储相同。

***11.9 缓存

反向代理或CDN（Content Delivery Network，内容分发网络），是很好的使用代理服务器缓存的例子。服务器端缓存，是由服务器来负责处理缓存，可能会使用像Redis或Memcache这样的系统，也可能是一个简单的内存缓存。

对于那些提供高度可缓存数据的服务，从设计上来讲，源服务本身就只能处理一小部分的流量，因为大多数请求已经被源服务前面的缓存处理了。如果我们突然得到一个晴天霹雳的消息，由于整个缓存区消失了，源服务就会接收到远大于其处理能力的请求。在这种情况下，保护源服务的一种方式是，在第一时间就不要对源服务发起请求。相反，如图11-7所示，在需要时源服务本身会异步地填充缓存。如果缓存请求失败，会触发一个给源服务的事件，提醒它需要重新填充缓存。所以如果整个分片消失了，我们可以在后台重建缓存。可以阻塞请求直到区域被重新填充，但这可能会使缓存本身的争用，从而导致一些问题。更合适的是，如果想优先保持系统的稳定，我们可以让原始请求失败，但要快速地失败。[插图]图11-7：保护源服务，在后台异步重建缓存在某些情况下这种方法可能没有意义，但当系统的一部分发生故障时，它是确保系统仍然可用的一种方式。让请求快速失败，确保不占用资源或增加延迟，我们避免了级联下游服务导致的缓存故障，并给自己一个恢复的机会。

缓存可以很强大，但是你需要了解数据从数据源到终点的完整缓存路径，从而真正理解它的复杂性以及使它出错的原因。

***11.11 CAP定理

在分布式系统中有三方面需要彼此权衡：一致性（consistency）、可用性（availability）和分区容忍性（partition tolerance）。具体地说，这个定理告诉我们最多只能保证三个中的两个。

一致性是当访问多个节点时能得到同样的值。可用性意味着每个请求都能获得响应。分区容忍性是指集群中的某些节点在无法联系后，集群整体还能继续进行服务的能力。

现实情况是，即使我们没有数据库节点之间的网络故障，数据复制也不是立即发生的。正如前面提到的，系统放弃一致性以保证分区容忍性和可用性的这种做法，被称为最终一致性；也就是说，我们希望在将来的某个时候，所有节点都能看到更新后的数据，但它不会马上发生，所以我们必须清楚用户将看到失效数据的可能性

现在在分区情况下，如果数据库节点不能彼此通信，则它们无法协调以保证一致性。由于无法保证一致性，所以我们唯一的选择就是拒绝响应请求。换句话说，我们牺牲了可用性。系统是一致的和分区容忍的，即CP。在这种模式下，我们的服务必须考虑如何做功能降级，直到分区恢复以及数据库节点之间可以重新同步。

如果系统没有分区容忍性，就不能跨网络运行。换句话说，需要在本地运行一个单独的进程。所以，CA系统在分布式系统中根本是不存在的。

哪个是正确的，AP还是CP？好吧，现实中要视情况而定。因为我们知道，在人们构建系统的过程中需要权衡。我们知道AP系统扩展更容易，而且构建更简单，而CP系统由于要支持分布式一致性会遇到更多的挑战，需要更多的工作

即使对于一致性或可用性而言，也可以有选择地部分采用。许多系统允许我们更精细地做权衡。例如，Cassandra允许为每个调用做不同的权衡。因此如果需要严格的一致性，我可以在执行一个读取时，保持其阻塞直到所有副本回应确认数据是一致的，或直到特定数量的副本做出回应，或仅仅是一个节点做出回应。

***11.12 服务发现

我见过的解决方案，都会把事情分成两部分进行处理。首先，它们提供了一些机制，让一个实例注册并告诉所有人：“我在这里！”其次，它们提供了一种方法，一旦服务被注册就可以找到它。然后，当考虑在一个不断销毁和部署新实例的环境中，服务发现会变得更复杂。理想情况下，我们希望无论选择哪种解决方案，它都应该可以解决这些问题。

***11.13 动态服务注册

像许多相似类型的系统，Zookeeper依赖于在集群中运行大量的节点，以提供各种保障。这意味着，你至少应该运行三个Zookeeper节点。Zookeeper的大部分优点，围绕在确保数据在这些节点间安全地复制，以及当节点故障后仍能保持一致性上。

Zookeeper的核心是提供了一个用于存储信息的分层命名空间。客户端可以在此层次结构中，插入新的节点，更改或查询它们。此外，它们可以在节点上添加监控功能，以便当信息更改时节点能够得到通知。这意味着，我们可以在这个结构中存储服务位置的信息，并且可以作为一个客户端来接收更改消息。Zookeeper通常被用作通用配置存储，因此你也可以存储与特定服务相关的配置，这可以帮助你完成类似动态更改日志级别，或关闭正在运行的系统特性这样的任务。我个人倾向于不使用Zookeerp这样的系统作为配置源，因为我认为这使得在给定服务中定位变得更加困难。

和Zookeeper一样，Consul（http://www.consul.io/）也支持配置管理和服务发现。但它比Zookeeper更进一步，为这些关键使用场景提供了更多的支持。例如，它为服务发现提供一个HTTP接口。Consul提供的杀手级特性之一是，它实际上提供了现成的DNS服务器。具体来说，对于指定的名字，它能提供一条SRV记录，其中包含IP和端口。这意味着，如果系统的一部分已经在使用DNS，并且支持SRV记录，你就可以直接开始使用Consul，而无需对现有系统做任何更改。

***11.14 文档服务

Swagger让你描述API，产生一个很友好的Web用户界面，使你可以查看文档并通过Web浏览器与API交互。能够直接执行请求是一个非常棒的特性。例如，你可以定义POST模板，明确微服务期望的内容是什么样的。

***11.16 小结

推荐Nygard的优秀图书Release It!。在书里他分享了一系列关于系统故障的故事，以及一些处理它们的模式。这本书很值得一读（事实上，我甚至认为它应该成为构建任何规模化系统的必读书籍

***12.1 微服务的原则

[插图]图12-1：微服务的原则

经验表明，围绕业务的限界上下文定义的接口，比围绕技术概念定义的接口更加稳定。针对系统如何工作这个领域进行建模，不仅可以帮助我们形成更稳定的接口，也能确保我们能够更好地反映业务流程的变化。使用限界上下文来定义可能的领域边界。

为了使一个服务独立于其他服务，最大化独自演化的能力，隐藏实现细节至关重要。限界上下文建模在这方面可以提供帮助，因为它可以帮助我们关注哪些模型应该共享，哪些应该隐藏。服务还应该隐藏它们的数据库，以避免陷入数据库耦合，这在传统的面向服务的架构中也是最常见的一种耦合类型。使用数据泵（data pump）或事件数据泵（event data pump），将跨多个服务的数据整合到一起，以实现报表的功能

像企业服务总线或服务编配系统这样的方案，会导致业务逻辑的中心化和哑服务，应该避免使用它们。使用协同来代替编排或哑中间件，使用智能端点（smart endpoint）确保相关的逻辑和数据，在服务限界内能保持服务的内聚性

通过采用单服务单主机模式，可以减少部署一个服务引发的副作用，比如影响另一个完全不相干的服务。请考虑使用蓝/绿部署或金丝雀部署技术，区分部署和发布，降低发布出错的风险。使用消费者驱动的契约测试，在破坏性的更改发生前捕获它们。

请记住，你可以更改单个服务，然后把它部署到生产环境，无需联动地部署其他任何服务，这应该是常态，而不是例外。你的消费者应该自己决定何时更新，你需要适应他们。

如果我们心中持有反脆弱的信条，预期在任何地方都会发生故障，这说明我们正走在正确的路上。请确保正确设置你的超时，了解何时及如何使用舱壁和断路器，来限制故障组件的连带影响。

高度可观察我们不能依靠观察单一服务实例，或一台服务器的行为，来看系统是否运行正常。相反，我们需要从整体上看待正在发生的事情。通过注入合成事务到你的系统，模拟真实用户的行为，从而使用语义监控来查看系统是否运行正常。聚合你的日志和数据，这样当你遇到问题时，就可以深入分析原因。而当需要重现令人讨厌的问题，或仅仅查看你的系统在生产环境是如何交互时，关联标识可以帮助你跟踪系统间的调用。

***12.2 什么时候你不应该使用微服务

从头开发也很具有挑战性。不仅仅因为其领域可能是新的，还因为对已有东西进行分类，要比对不存在的东西进行分类要容易得多！因此，请再次考虑首先构建单块系统，当稳定以后再进行拆分。
